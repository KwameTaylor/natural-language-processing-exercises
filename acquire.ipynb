{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Acquisition Web Scraping Exercises\n",
    "### Kwame V. Taylor\n",
    "\n",
    "By the end of this exercise, you should have a file named ```acquire.py``` that contains the specified functions. If you wish, you may break your work into separate files for each website (e.g. ```acquire_codeup_blog.py``` and ```acquire_news_articles.py```), but the end function should be present in ```acquire.py``` (that is, ```acquire.py``` should import ```get_blog_articles``` from the ```acquire_codeup_blog``` module.)\n",
    "\n",
    "1. Codeup Blog Articles\n",
    "\n",
    "Scrape the article text from the following pages:\n",
    "\n",
    "* https://codeup.com/codeups-data-science-career-accelerator-is-here/\n",
    "* https://codeup.com/data-science-myths/\n",
    "* https://codeup.com/data-science-vs-data-analytics-whats-the-difference/\n",
    "* https://codeup.com/10-tips-to-crush-it-at-the-sa-tech-job-fair/\n",
    "* https://codeup.com/competitor-bootcamps-are-closing-is-the-model-in-danger/\n",
    "\n",
    "Encapsulate your work in a function named ```get_blog_articles``` that will return a list of dictionaries, with each dictionary representing one article. The shape of each dictionary should look like this:\n",
    "\n",
    ">```\n",
    ">{\n",
    ">    'title': 'the title of the article',\n",
    ">    'content': 'the full text content of the article'\n",
    ">}\n",
    ">```\n",
    "\n",
    "Plus any additional properties you think might be helpful.\n",
    "\n",
    "**Bonus:**\n",
    "Scrape the text of all the articles linked on codeup's blog page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from requests import get\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://codeup.com/codeups-data-science-career-accelerator-is-here/'\n",
    "headers = {'User-Agent': 'Codeup Data Science'} # Some websites don't accept the pyhon-requests default user-agent\n",
    "response = get(url, headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html><html lang=\"en-US\"><head >\t<meta charset=\"UTF-8\" />\n",
      "\t<meta name=\"viewport\" content=\"width=device-width, initial-scale=1\" />\n",
      "\t<style type=\"text/css\" id=\"nab-alternative-loader-style\"></style>\n",
      "<script type=\"text/javascript\" id=\"nelio-ab-testing-kickoff\">/* <![CDATA[ */( function() { var ua = window.navigator.userAgent || ''; if ( -1 !== ua.indexOf( 'MSIE ' ) || -1 !== ua.indexOf( 'Tri\n"
     ]
    }
   ],
   "source": [
    "print(response.text[:400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a soup variable holding the response content\n",
    "soup = BeautifulSoup(response.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bs4.BeautifulSoup'>\n"
     ]
    }
   ],
   "source": [
    "print(type(soup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The rumors are true! The time has arrived. Codeup has officially opened applications to our new Data Science career accelerator, with only 25 seats available! This immersive program is one of a kind in San Antonio, and will help you land a job in\\xa0Glassdoorâ€™s #1 Best Job in America.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title = soup.find('h1').text\n",
    "content = soup.find('p').text\n",
    "articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_blog_articles(urls):\n",
    "    articles = []\n",
    "\n",
    "    for url in urls:\n",
    "        # Make request and soup object\n",
    "        soup = make_soup(url)\n",
    "\n",
    "        # get title and paragraph/content\n",
    "        title = soup.find('h1').text\n",
    "        content = soup.find('p').text\n",
    "\n",
    "        # store articles\n",
    "        article = {'title': title, 'content': content}\n",
    "        articles.append(article)\n",
    "            \n",
    "    # save as a DataFrame\n",
    "    df = pd.DataFrame(articles)\n",
    "    \n",
    "    return df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
